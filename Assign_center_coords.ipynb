{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get most representative coordinates out of a dataframe and store them in S3.\n",
    "Creates a dataframe of lat/lon values with mean prices which enables encoding\n",
    "coordinates to categorical and interesting feature engineering at later steps.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging as log\n",
    "import tempfile\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "\n",
    "import columns\n",
    "from common import (\n",
    "    DATA_TYPES,\n",
    "    CLEAN_DATA_PATH,\n",
    "    COORDS_MAP_MODELS_PATH,\n",
    "    logs_conf,\n",
    ")\n",
    "from pipelines.utils import (\n",
    "    closest_point,\n",
    "    unzip_coord_series_to_lon_and_lat,\n",
    "    add_zipped_coords_column,\n",
    ")\n",
    "from s3_client import s3_client\n",
    "\n",
    "# max distance (in km) between coordinates to get \"clustered\"\n",
    "EPSILON = 3\n",
    "# min samples per cluster\n",
    "MIN_SAMPLES = 1\n",
    "KMS_PER_RADIAN = 6371.0088\n",
    "\"\"\"\"\n",
    "TODO:\n",
    "- Replace s3_client calls with local read/write calls.\n",
    "- Move imported functions to local ones.\n",
    "- Clean up DBScan a little and do a little reading.\n",
    "\n",
    "\"\"\"\"\n",
    "s3_client = s3_client()\n",
    "\n",
    "def coords_map_task(data_type):\n",
    "    log.info(f\"Starting coords encoding map task for {data_type} data.\")\n",
    "    newest_df = s3_client.read_newest_df_from_s3(CLEAN_DATA_PATH, dtype=data_type)\n",
    "    cols = newest_df.columns\n",
    "    if columns.LON not in cols or columns.LAT not in cols:\n",
    "        log.warning(\"Missing coordinates. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    coords_map = get_coords_map(newest_df, data_type)\n",
    "\n",
    "    s3_client.upload_df_to_s3_with_timestamp(coords_map,\n",
    "                                             s3_path=COORDS_MAP_MODELS_PATH,\n",
    "                                             keyword='coords_map',\n",
    "                                             dtype=data_type,\n",
    "                                             )\n",
    "    log.info(f\"Finished coords encoding map task for {data_type} data.\")\n",
    "\n",
    "\n",
    "def get_coords_map(df, data_type):\n",
    "    # remove \"artificial\" duplicates\n",
    "    df_unduped = df.drop_duplicates(subset=[columns.LON, columns.LAT], keep=\"last\")\n",
    "    repr_coords_df = get_repr_points(df_unduped)\n",
    "    coords_tuple_colname = \"coords_tuple\"\n",
    "    repr_coords_df = add_zipped_coords_column(repr_coords_df, coords_tuple_colname)\n",
    "    df = add_zipped_coords_column(df, coords_tuple_colname)\n",
    "    # assign a closest point\n",
    "    df[\"coords_closest_tuple\"] = [\n",
    "        closest_point(x, list(repr_coords_df[coords_tuple_colname]))\n",
    "        for x in df[coords_tuple_colname]\n",
    "    ]\n",
    "    coords_encoding_map = (\n",
    "        df.loc[:, [\"coords_closest_tuple\", columns.PRICE_M2, columns.PRICE]]\n",
    "        .groupby(\"coords_closest_tuple\", as_index=False)\n",
    "        .mean()\n",
    "        .sort_values(by=columns.PRICE_M2)\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns={\n",
    "            columns.PRICE_M2: columns.CLUSTER_MEAN_PRICE_M2,\n",
    "            columns.PRICE: columns.CLUSTER_MEAN_PRICE,\n",
    "        })\n",
    "        .pipe(unzip_coord_series_to_lon_and_lat, \"coords_closest_tuple\")\n",
    "    )\n",
    "    coords_encoding_map[columns.CLUSTER_ID] = coords_encoding_map.index + 1\n",
    "    return coords_encoding_map\n",
    "\n",
    "\n",
    "def get_repr_points(lon_lat_df):\n",
    "    \"\"\"\n",
    "    Get's lon's and lat's representative for a dataframe with lon and lat\n",
    "    values. For details see:\n",
    "    https://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/\n",
    "    \"\"\"\n",
    "    coords = lon_lat_df[[columns.LAT, columns.LON]].to_numpy()\n",
    "\n",
    "    epsilon = EPSILON / KMS_PER_RADIAN\n",
    "\n",
    "    log.info(\"Starting DBScan alghorithm ...\")\n",
    "    db = DBSCAN(\n",
    "        eps=epsilon, min_samples=MIN_SAMPLES, algorithm=\"ball_tree\", metric=\"haversine\"\n",
    "    ).fit(np.radians(coords))\n",
    "\n",
    "    cluster_labels = db.labels_\n",
    "    num_clusters = len(set(cluster_labels))\n",
    "    clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)])\n",
    "    centermost_points = list(clusters.map(get_centermost_point))\n",
    "    log.info(f\"DBScan algoritm found {num_clusters} clusters.\")\n",
    "\n",
    "    return pd.DataFrame(centermost_points, columns=[columns.LAT, columns.LON])\n",
    "\n",
    "\n",
    "def get_centermost_point(cluster):\n",
    "    \"\"\"\n",
    "    Get the most \"center\" point for a cluster according to DBscan.\n",
    "    \"\"\"\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
